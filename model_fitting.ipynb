{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open data. All ECG have a lengh of 186. All the samples are cropped, downsampled and padded with zeroes if necessary to the fixed dimension of 188. Last column has the classification of the ecg : data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data.load_images_ecg()\n",
    "mitbih = data['mitbih_train']\n",
    "ptbdb = data['ptbdb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we checked in data_analysis, we have unbalenced data.\n",
    "We will try first to train a model with the dataset as it is, check if there's a problem with unbalanced data, and try different approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#Prepare samples to train of mitbih\n",
    "mitbid_ecg = mitbih.iloc[:,:186]\n",
    "mitbid_classification = mitbih.iloc[:,-1]\n",
    "x_train = mitbid_ecg\n",
    "y_train = to_categorical(mitbid_classification)\n",
    "x_test = data['mitbih_test'].iloc[:,:186]\n",
    "y_test = to_categorical(data['mitbih_test'].iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/80\n",
      "WARNING:tensorflow:From c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "685/685 [==============================] - 5s 4ms/step - loss: 0.2720 - accuracy: 0.9250 - val_loss: 0.1789 - val_accuracy: 0.9524\n",
      "Epoch 2/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.1449 - accuracy: 0.9599 - val_loss: 0.1516 - val_accuracy: 0.9571\n",
      "Epoch 3/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.1178 - accuracy: 0.9668 - val_loss: 0.1277 - val_accuracy: 0.9641\n",
      "Epoch 4/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.1029 - accuracy: 0.9710 - val_loss: 0.1302 - val_accuracy: 0.9606\n",
      "Epoch 5/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0952 - accuracy: 0.9730 - val_loss: 0.1264 - val_accuracy: 0.9649\n",
      "Epoch 6/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0911 - accuracy: 0.9739 - val_loss: 0.1058 - val_accuracy: 0.9715\n",
      "Epoch 7/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0827 - accuracy: 0.9760 - val_loss: 0.0980 - val_accuracy: 0.9728\n",
      "Epoch 8/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0779 - accuracy: 0.9776 - val_loss: 0.0971 - val_accuracy: 0.9730\n",
      "Epoch 9/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0732 - accuracy: 0.9786 - val_loss: 0.0883 - val_accuracy: 0.9754\n",
      "Epoch 10/80\n",
      "685/685 [==============================] - 3s 5ms/step - loss: 0.0703 - accuracy: 0.9790 - val_loss: 0.0900 - val_accuracy: 0.9761\n",
      "Epoch 11/80\n",
      "685/685 [==============================] - 3s 5ms/step - loss: 0.0673 - accuracy: 0.9802 - val_loss: 0.1028 - val_accuracy: 0.9715\n",
      "Epoch 12/80\n",
      "685/685 [==============================] - 4s 5ms/step - loss: 0.0682 - accuracy: 0.9797 - val_loss: 0.0917 - val_accuracy: 0.9747\n",
      "Epoch 13/80\n",
      "685/685 [==============================] - 4s 6ms/step - loss: 0.0601 - accuracy: 0.9819 - val_loss: 0.0869 - val_accuracy: 0.9769\n",
      "Epoch 14/80\n",
      "685/685 [==============================] - 3s 5ms/step - loss: 0.0573 - accuracy: 0.9825 - val_loss: 0.0936 - val_accuracy: 0.9753\n",
      "Epoch 15/80\n",
      "685/685 [==============================] - 4s 5ms/step - loss: 0.0555 - accuracy: 0.9828 - val_loss: 0.0862 - val_accuracy: 0.9779\n",
      "Epoch 16/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0529 - accuracy: 0.9838 - val_loss: 0.0873 - val_accuracy: 0.9781\n",
      "Epoch 17/80\n",
      "685/685 [==============================] - 3s 5ms/step - loss: 0.0510 - accuracy: 0.9845 - val_loss: 0.0879 - val_accuracy: 0.9779\n",
      "Epoch 18/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0493 - accuracy: 0.9849 - val_loss: 0.0841 - val_accuracy: 0.9785\n",
      "Epoch 19/80\n",
      "685/685 [==============================] - 3s 5ms/step - loss: 0.0474 - accuracy: 0.9852 - val_loss: 0.0916 - val_accuracy: 0.9778\n",
      "Epoch 20/80\n",
      "685/685 [==============================] - 4s 7ms/step - loss: 0.0458 - accuracy: 0.9854 - val_loss: 0.0892 - val_accuracy: 0.9784\n",
      "Epoch 21/80\n",
      "685/685 [==============================] - 4s 6ms/step - loss: 0.0438 - accuracy: 0.9862 - val_loss: 0.0837 - val_accuracy: 0.9792\n",
      "Epoch 22/80\n",
      "685/685 [==============================] - 3s 5ms/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.0871 - val_accuracy: 0.9794\n",
      "Epoch 23/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0408 - accuracy: 0.9870 - val_loss: 0.0825 - val_accuracy: 0.9793\n",
      "Epoch 24/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0414 - accuracy: 0.9867 - val_loss: 0.0842 - val_accuracy: 0.9796\n",
      "Epoch 25/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0388 - accuracy: 0.9878 - val_loss: 0.0826 - val_accuracy: 0.9794\n",
      "Epoch 26/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0376 - accuracy: 0.9879 - val_loss: 0.0851 - val_accuracy: 0.9802\n",
      "Epoch 27/80\n",
      "685/685 [==============================] - 2s 4ms/step - loss: 0.0363 - accuracy: 0.9884 - val_loss: 0.0811 - val_accuracy: 0.9814\n",
      "Epoch 28/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0357 - accuracy: 0.9886 - val_loss: 0.0864 - val_accuracy: 0.9795\n",
      "Epoch 29/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0353 - accuracy: 0.9887 - val_loss: 0.0912 - val_accuracy: 0.9763\n",
      "Epoch 30/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0329 - accuracy: 0.9891 - val_loss: 0.0904 - val_accuracy: 0.9800\n",
      "Epoch 31/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0325 - accuracy: 0.9893 - val_loss: 0.0839 - val_accuracy: 0.9794\n",
      "Epoch 32/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0312 - accuracy: 0.9900 - val_loss: 0.0877 - val_accuracy: 0.9804\n",
      "Epoch 33/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0309 - accuracy: 0.9899 - val_loss: 0.0907 - val_accuracy: 0.9811\n",
      "Epoch 34/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0290 - accuracy: 0.9904 - val_loss: 0.0946 - val_accuracy: 0.9795\n",
      "Epoch 35/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0307 - accuracy: 0.9896 - val_loss: 0.0902 - val_accuracy: 0.9813\n",
      "Epoch 36/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0270 - accuracy: 0.9909 - val_loss: 0.0901 - val_accuracy: 0.9794\n",
      "Epoch 37/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0262 - accuracy: 0.9913 - val_loss: 0.0997 - val_accuracy: 0.9795\n",
      "Epoch 38/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0277 - accuracy: 0.9911 - val_loss: 0.1052 - val_accuracy: 0.9791\n",
      "Epoch 39/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0255 - accuracy: 0.9913 - val_loss: 0.1002 - val_accuracy: 0.9804\n",
      "Epoch 40/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0291 - accuracy: 0.9907 - val_loss: 0.0930 - val_accuracy: 0.9809\n",
      "Epoch 41/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 0.0981 - val_accuracy: 0.9787\n",
      "Epoch 42/80\n",
      "685/685 [==============================] - 4s 5ms/step - loss: 0.0232 - accuracy: 0.9920 - val_loss: 0.1000 - val_accuracy: 0.9746\n",
      "Epoch 43/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.0967 - val_accuracy: 0.9799\n",
      "Epoch 44/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0218 - accuracy: 0.9925 - val_loss: 0.0957 - val_accuracy: 0.9810\n",
      "Epoch 45/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0224 - accuracy: 0.9926 - val_loss: 0.1047 - val_accuracy: 0.9801\n",
      "Epoch 46/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0223 - accuracy: 0.9924 - val_loss: 0.1026 - val_accuracy: 0.9802\n",
      "Epoch 47/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0207 - accuracy: 0.9929 - val_loss: 0.0960 - val_accuracy: 0.9809\n",
      "Epoch 48/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0212 - accuracy: 0.9927 - val_loss: 0.0993 - val_accuracy: 0.9814\n",
      "Epoch 49/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 0.1049 - val_accuracy: 0.9810\n",
      "Epoch 50/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0203 - accuracy: 0.9931 - val_loss: 0.1155 - val_accuracy: 0.9802\n",
      "Epoch 51/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 0.0992 - val_accuracy: 0.9819\n",
      "Epoch 52/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0190 - accuracy: 0.9934 - val_loss: 0.1043 - val_accuracy: 0.9813\n",
      "Epoch 53/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.1085 - val_accuracy: 0.9799\n",
      "Epoch 54/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0191 - accuracy: 0.9936 - val_loss: 0.1152 - val_accuracy: 0.9815\n",
      "Epoch 55/80\n",
      "685/685 [==============================] - 3s 5ms/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.1043 - val_accuracy: 0.9817\n",
      "Epoch 56/80\n",
      "685/685 [==============================] - 5s 8ms/step - loss: 0.0193 - accuracy: 0.9937 - val_loss: 0.1077 - val_accuracy: 0.9803\n",
      "Epoch 57/80\n",
      "685/685 [==============================] - 4s 6ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.1102 - val_accuracy: 0.9802\n",
      "Epoch 58/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0163 - accuracy: 0.9943 - val_loss: 0.1125 - val_accuracy: 0.9815\n",
      "Epoch 59/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.1110 - val_accuracy: 0.9804\n",
      "Epoch 60/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0168 - accuracy: 0.9943 - val_loss: 0.1036 - val_accuracy: 0.9815\n",
      "Epoch 61/80\n",
      "685/685 [==============================] - 2s 4ms/step - loss: 0.0174 - accuracy: 0.9940 - val_loss: 0.1367 - val_accuracy: 0.9777\n",
      "Epoch 62/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0147 - accuracy: 0.9946 - val_loss: 0.1144 - val_accuracy: 0.9813\n",
      "Epoch 63/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0162 - accuracy: 0.9945 - val_loss: 0.1104 - val_accuracy: 0.9804\n",
      "Epoch 64/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0161 - accuracy: 0.9945 - val_loss: 0.1074 - val_accuracy: 0.9793\n",
      "Epoch 65/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0162 - accuracy: 0.9942 - val_loss: 0.1319 - val_accuracy: 0.9803\n",
      "Epoch 66/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0143 - accuracy: 0.9953 - val_loss: 0.1117 - val_accuracy: 0.9818\n",
      "Epoch 67/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.1136 - val_accuracy: 0.9813\n",
      "Epoch 68/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0140 - accuracy: 0.9950 - val_loss: 0.1328 - val_accuracy: 0.9790\n",
      "Epoch 69/80\n",
      "685/685 [==============================] - 6s 8ms/step - loss: 0.0140 - accuracy: 0.9949 - val_loss: 0.1266 - val_accuracy: 0.9816\n",
      "Epoch 70/80\n",
      "685/685 [==============================] - 7s 10ms/step - loss: 0.0137 - accuracy: 0.9953 - val_loss: 0.1276 - val_accuracy: 0.9815\n",
      "Epoch 71/80\n",
      "685/685 [==============================] - 4s 5ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.1313 - val_accuracy: 0.9799\n",
      "Epoch 72/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.1238 - val_accuracy: 0.9823\n",
      "Epoch 73/80\n",
      "685/685 [==============================] - 4s 6ms/step - loss: 0.0116 - accuracy: 0.9961 - val_loss: 0.1220 - val_accuracy: 0.9823\n",
      "Epoch 74/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0137 - accuracy: 0.9952 - val_loss: 0.1235 - val_accuracy: 0.9806\n",
      "Epoch 75/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.1207 - val_accuracy: 0.9815\n",
      "Epoch 76/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0113 - accuracy: 0.9961 - val_loss: 0.1215 - val_accuracy: 0.9814\n",
      "Epoch 77/80\n",
      "685/685 [==============================] - 2s 4ms/step - loss: 0.0138 - accuracy: 0.9950 - val_loss: 0.1198 - val_accuracy: 0.9815\n",
      "Epoch 78/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.1347 - val_accuracy: 0.9800\n",
      "Epoch 79/80\n",
      "685/685 [==============================] - 3s 4ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.1242 - val_accuracy: 0.9822\n",
      "Epoch 80/80\n",
      "685/685 [==============================] - 2s 3ms/step - loss: 0.0105 - accuracy: 0.9965 - val_loss: 0.1260 - val_accuracy: 0.9813\n",
      "evaluation: \n",
      "685/685 [==============================] - 2s 2ms/step - loss: 0.1260 - accuracy: 0.9813\n"
     ]
    }
   ],
   "source": [
    "# Let's try to create a simple 2 dense layers model with a final classification with softmax (model_dense_layers_v1)\n",
    "import models\n",
    "model = models.model_dense_layers_v1()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "his = model.fit(x_train, y_train, batch_size=128, validation_data=(x_test, y_test) , epochs=80, verbose=True)\n",
    "print('evaluation: ' )\n",
    "loss , acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 2s 2ms/step\n",
      "Classe:0 | Total samples:18118 | Correct predictions: 17986 | 0.99271442764102\n",
      "Classe:1 | Total samples:556 | Correct predictions: 426 | 0.7661870503597122\n",
      "Classe:2 | Total samples:1448 | Correct predictions: 1373 | 0.9482044198895028\n",
      "Classe:3 | Total samples:162 | Correct predictions: 124 | 0.7654320987654321\n",
      "Classe:4 | Total samples:1608 | Correct predictions: 1574 | 0.9788557213930348\n"
     ]
    }
   ],
   "source": [
    "import evaluation_functions\n",
    "\n",
    "# Get predictions \n",
    "predictions = model.predict(x_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "ground_truth = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Understand number of classes the model learned more.\n",
    "evaluation_functions.print_categorical_acc(ground_truth=ground_truth, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS we see, the Classe 1 and classe 3 that have lower representation, have a accuracy of 76%\n",
    "Let's try to oversample the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "ada = ADASYN(sampling_strategy='all')\n",
    "\n",
    "x_train_resample, y_train_resample = ada.fit_resample(x_train, np.argmax(y_train, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 5) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mmodel_dense_layers_v1()\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m his \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_resample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_resample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation: \u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[0;32m      6\u001b[0m loss , acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[1;32mc:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8ilhp95l.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\maria\\AppData\\Local\\anaconda3\\envs\\env_datascience\\lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 5) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Let's try again the same exacct model, with a more balanced data\n",
    "y_train_resample = to_categorical(y_train_resample)\n",
    "model = models.model_dense_layers_v1()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "his = model.fit(x_train_resample, y_train_resample, batch_size=128, validation_data=(x_test, y_test) , epochs=80, verbose=True)\n",
    "print('evaluation: ' )\n",
    "loss , acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Get predictions \n",
    "predictions = model.predict(x_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "ground_truth = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Understand number of classes the model learned more.\n",
    "evaluation_functions.print_categorical_acc(ground_truth=ground_truth, predictions=predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
